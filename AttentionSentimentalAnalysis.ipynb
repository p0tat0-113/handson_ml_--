{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T05:40:02.065305Z",
     "start_time": "2025-02-23T05:40:02.061910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "141e8a90bc2d67ef",
   "outputs": [],
   "execution_count": 112
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# SinusoidalPositionalEncoding",
   "id": "cedf5e0124053e23"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<img src=\"image/sinusoidal.png\" width=\"300\"/>",
   "id": "cd3b986324b5ddf9"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-23T05:40:02.082975Z",
     "start_time": "2025-02-23T05:40:02.073475Z"
    }
   },
   "source": [
    "class SinusoidalPositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_seq_len, d_model, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # 입력 마스크를 다음 층으로 자동으로 전파\n",
    "        self.support_masking = True\n",
    "\n",
    "        if d_model % 2 != 0:\n",
    "            raise ValueError(\"d_model must be even to use sinusoidal positional encoding.\")\n",
    "\n",
    "        # 생성자에서 `모델의 최대 입력 시퀀스` X `임베딩 차원` 크기의 위치 인코딩 행렬을 미리 계산한다.\n",
    "\n",
    "        # 위치와 차원에 대한 격자(grid) 생성\n",
    "        position = np.arange(max_seq_len)[:, np.newaxis]\n",
    "        div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "\n",
    "        # 3차원 포지셔널 인코딩 행렬 생성\n",
    "        pos_emb = np.zeros((1, max_seq_len, d_model))\n",
    "\n",
    "        # sin 함수를 짝수 인덱스에 적용\n",
    "        pos_emb[0, :, 0::2] = np.sin(position * div_term)\n",
    "        # cos 함수를 홀수 인덱스에 적용\n",
    "        pos_emb[0, :, 1::2] = np.cos(position * div_term)\n",
    "\n",
    "        # tensorflow 상수로 변환하고 float32로 타입 지정\n",
    "        self.pos_emb = tf.constant(pos_emb, dtype=tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_max_seq_len = tf.shape(inputs)[1] # 인코더로 입력된 시퀀스의 길이를 동적으로 계산한다. [batch_size, sequence_length, embed_size]\n",
    "        return inputs + self.pos_emb[:, :batch_max_seq_len] # 필요한 만큼 위치인코딩을 잘라서 더한다."
   ],
   "outputs": [],
   "execution_count": 113
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Scaled Dot-Product Attention",
   "id": "ce518d59c45ae842"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<img src=\"image/Screenshot 2025-02-20 at 8.43.14 PM.png\" width=\"500\"/>",
   "id": "705a27f5ac39e504"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T05:40:02.093754Z",
     "start_time": "2025-02-23T05:40:02.090091Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ScaledDotProductAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.support_masking = True # 입력 마스크를 다음 층으로 전파.\n",
    "\n",
    "    def call(self, q, k, v, mask=None):\n",
    "        # 쿼리와 키의 점곱으로 어텐션 스코어 계산\n",
    "        # 이 층의 입력으로는 h개의 헤드로 쪼개진 상태의 [batch, n_heads, seq_len, d_k] shape의 텐서가 들어온다.\n",
    "        # 같은 배치, 같은 헤드의 쿼리와 키 끼리 행렬곱을 수행한다.\n",
    "\n",
    "        # k의 전치는 마지막 2개 차원에 대해서만 수행해야 한다.\n",
    "        attention_score = tf.matmul(q, tf.transpose(k, perm=[0,1,3,2]))\n",
    "\n",
    "        # 패딩 토큰 부분에 극단적인 음수값 부여, 소프트맥스 함수를 통과했을 때 0에 수렴한다.\n",
    "        if mask is not None:\n",
    "            # mask의 shape는 [batch, 1, 1, key_seq_len] 또는 브로드캐스팅 가능한 형태여야 함\n",
    "            # ID가 0인 위치가 False다.\n",
    "            attention_score += (mask - 1) * 1e9\n",
    "\n",
    "        # 스케일\n",
    "        # 마지막 차원의 제곱근으로 나누어서 스케일링 한다. 마지막 차원은 한 헤드의 차원과 같다.\n",
    "        d_k = tf.cast(tf.shape(k)[-1], dtype=tf.float32)\n",
    "        attention_score = attention_score / tf.math.sqrt(d_k)\n",
    "\n",
    "        # 소프트맥스 함수 통과\n",
    "        attention_weight = tf.keras.activations.softmax(attention_score)\n",
    "\n",
    "        # weighted sum, value에 가중치를 곱한다.\n",
    "        output = tf.matmul(attention_weight, v)\n",
    "\n",
    "        return output"
   ],
   "id": "bf4326c3f3247696",
   "outputs": [],
   "execution_count": 114
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Multi Head Attention",
   "id": "6fe67c5cadaa0dd9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<p float=\"left\">\n",
    "  <img src=\"https://wikidocs.net/images/page/159310/mha_img_original.png\" width=\"500\"/>\n",
    "  <img src=\"https://wikidocs.net/images/page/159310/mha_visualization-930x1030.png\" width=\"300\"/>\n",
    "</p>"
   ],
   "id": "5dc138a74da0362b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T05:40:02.115940Z",
     "start_time": "2025-02-23T05:40:02.103460Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model ,n_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.support_masking=True # 다음 층으로 마스킹 자동 전파\n",
    "\n",
    "        # Wq Wk Wv 선형 변환 층\n",
    "        # 입력 텐서가 이 선형 변환 층을 통과한 다음에 여러개의 헤드로 쪼개진다.\n",
    "        self.Wq = tf.keras.layers.Dense(d_model)\n",
    "        self.Wk = tf.keras.layers.Dense(d_model)\n",
    "        self.Wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        # n_heads개의 헤드로 쪼갰을 떄 d_k를 계산한다.\n",
    "        if d_model % n_heads != 0 : # 나누어 떨어지지 않으면 예외를 발생시킴\n",
    "            raise ValueError(f\"d_model % n_heads should be 0 d_model: {d_model}, n_heads: {n_heads}\")\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads # 한 헤드의 차원\n",
    "\n",
    "        # 마지막으로 concat된 행렬을 통과시키는 선형변환 층, 입력과 출력의 크기가 똑같아야 한다는 점을 명심하자.\n",
    "        self.Wo = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, q, k, v, mask=None): # 마스크를 입력으로 받음.\n",
    "        # 선형 변환\n",
    "        q = self.Wq(q)\n",
    "        k = self.Wk(k)\n",
    "        v = self.Wv(v)\n",
    "\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        # n_heads개의 헤드로 쪼개는 과정\n",
    "\n",
    "        # reshape, 마지막 차원 d_k를 (self.n_heads X self.d_k) 형태로 분리함.\n",
    "        # 시퀀스 길이는 -1로 표시, 자동계산된다.\n",
    "        q = tf.reshape(q, [batch_size, -1, self.n_heads, self.d_k])\n",
    "        k = tf.reshape(k, [batch_size, -1, self.n_heads, self.d_k])\n",
    "        v = tf.reshape(v, [batch_size, -1, self.n_heads, self.d_k])\n",
    "\n",
    "        # 차원 순서를 바꾼다. 같은 배치, 같은 헤드의 쿼리와 키 끼리 행렬곱하게 된다.\n",
    "        # 덕분에 매우 효율적으로 어텐션 연산을 수행할 수 있음.\n",
    "        q = tf.transpose(q, [0,2,1,3])\n",
    "        k = tf.transpose(k, [0,2,1,3])\n",
    "        v = tf.transpose(v, [0,2,1,3])\n",
    "\n",
    "        # scaled dot-product attention층에 집어넣어서 어텐션 연산을 수행한다.\n",
    "        attention_layer = ScaledDotProductAttention()\n",
    "        scaled_attention = attention_layer(q,k,v,mask) # 어텐션 층에 마스크를 넣어줌\n",
    "\n",
    "        # concat, 헤드를 쪼개는 과정을 반대로 해주면 된다.\n",
    "        scaled_attention = tf.transpose(scaled_attention, [0,2,1,3]) # 다시 [batch, seq_length, n_heads, d_k]\n",
    "        scaled_attention = tf.reshape(scaled_attention, [batch_size, -1, self.d_model]) # 다시 [batch, seq_length, d_model]\n",
    "\n",
    "        # 마지막 선형 변환\n",
    "        output = self.Wo(scaled_attention)\n",
    "\n",
    "        return output"
   ],
   "id": "47afbe783acb31aa",
   "outputs": [],
   "execution_count": 115
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Encoder Layer",
   "id": "387a2ff79fb02641"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<img src=\"https://wikidocs.net/images/page/159310/img_original_paper-726x1030.png\" width=\"300\"/>",
   "id": "7edcb0bc4c092c9d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T05:40:02.136403Z",
     "start_time": "2025-02-23T05:40:02.129837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer) :\n",
    "    def __init__(self, d_model, d_ff, n_heads, dropout, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.support_masking=True # 다음 층으로 마스킹 자동 전파\n",
    "\n",
    "        # attention layer normalization\n",
    "        self.attention_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        # ffn layer normalization\n",
    "        self.ffn_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        # ffn 레이어\n",
    "        self.ffn_layer1 = tf.keras.layers.Dense(d_ff)\n",
    "        self.activation = tf.keras.layers.Activation('relu')\n",
    "        self.ffn_layer2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        # 멀티 헤드 어텐션 층 정의\n",
    "        self.multi_head_attention_layer = MultiHeadAttention(d_model, n_heads)\n",
    "\n",
    "        # 정규화를 위한 Dropout층 정의\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout) # 어텐션에 사용\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout) # ffn에 사용\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        skip = inputs\n",
    "\n",
    "        # 멀티 헤드 어텐션 연산\n",
    "        attention = self.multi_head_attention_layer(inputs, inputs, inputs, mask)\n",
    "\n",
    "        # Dropout\n",
    "        attention = self.dropout1(attention)\n",
    "\n",
    "        # skip connection\n",
    "        attention = attention + skip\n",
    "\n",
    "        # layer normalization\n",
    "        attention = self.attention_norm(attention)\n",
    "\n",
    "        skip = attention\n",
    "\n",
    "        # ffn 통과\n",
    "        ffn_output = self.ffn_layer1(attention)\n",
    "        ffn_output = self.activation(ffn_output)\n",
    "        ffn_output = self.ffn_layer2(ffn_output)\n",
    "\n",
    "        # Dropout\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "\n",
    "        # skip connection\n",
    "        ffn_output = ffn_output + skip\n",
    "\n",
    "        # layer normalization\n",
    "        ffn_output = self.ffn_norm(ffn_output)\n",
    "\n",
    "        return ffn_output"
   ],
   "id": "b6798a0242ff2495",
   "outputs": [],
   "execution_count": 116
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Encoder",
   "id": "f7f0c91ef11dbadb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T05:40:02.581955Z",
     "start_time": "2025-02-23T05:40:02.142115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocab_size = 10000  # 어휘 사전 크기\n",
    "max_seq_len = 500 # 최대 시퀀스 길이\n",
    "d_model = 128\n",
    "d_ff = 512\n",
    "n_heads = 8\n",
    "dropout = 0.1\n",
    "\n",
    "# 사용자 정의 전처리 함수: 문자를 모두 소문자로 바꾸고, 구두점을 제거, [CLS]토큰을 추가\n",
    "def custom_standardize(text):\n",
    "    text = tf.strings.lower(text)  # Lowercase\n",
    "    text = tf.strings.regex_replace(text, r\"[^\\w\\s]\", \"\")  # Remove punctuation\n",
    "    return tf.strings.join([\"[CLS]\", text], separator=\" \") # Add [CLS] token\n",
    "\n",
    "text_vec_layer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_seq_len,\n",
    "    standardize=custom_standardize\n",
    ")\n",
    "\n",
    "# 모델 정의\n",
    "\n",
    "inputs = tf.keras.Input(shape=(1,), dtype=tf.string)  # 입력은 문자열, Keras의 Input 레이어에서 지정하는 shape 인자는 각 샘플의 shape을 나타내며, 배치 차원은 별도로 자동 추가\n",
    "\n",
    "# 텍스트 벡터화: [batch, seq_len] (정수 인덱스로 변환됨)\n",
    "x = text_vec_layer(inputs)\n",
    "\n",
    "# 임베딩, 패딩 토큰에 대해 마스킹 적용\n",
    "x = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=d_model, mask_zero=True)(x)\n",
    "\n",
    "# 위치 인코딩\n",
    "x = SinusoidalPositionalEncoding(max_seq_len=max_seq_len, d_model=d_model)(x)\n",
    "\n",
    "# 여러 EncoderLayer 적용\n",
    "x = EncoderLayer(d_model=d_model, d_ff=d_ff, n_heads=n_heads, dropout=dropout)(x)\n",
    "x = EncoderLayer(d_model=d_model, d_ff=d_ff, n_heads=n_heads, dropout=dropout)(x)\n",
    "\n",
    "# [CLS] 토큰의 벡터만 추출 (시퀀스의 첫 번째 토큰)\n",
    "cls_token = tf.keras.layers.Lambda(lambda t: t[:, 0, :])(x)\n",
    "\n",
    "# 최종 출력, 이진 분류라면 Sigmoid 사용\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(cls_token)\n",
    "\n",
    "# 모델 생성\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)"
   ],
   "id": "b56c139e186d9fc3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/barrett11357/coding/pycharm/handson_ml_--/.venv/lib/python3.9/site-packages/keras/src/layers/layer.py:938: UserWarning: Layer 'sinusoidal_positional_encoding_10' (of type SinusoidalPositionalEncoding) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 117
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T05:40:02.604207Z",
     "start_time": "2025-02-23T05:40:02.588376Z"
    }
   },
   "cell_type": "code",
   "source": "model.summary()",
   "id": "db82ab5262082d4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"functional_10\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_10\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_10 (\u001B[38;5;33mInputLayer\u001B[0m)     │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1\u001B[0m)              │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ text_vectorization_10           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m500\u001B[0m)            │             \u001B[38;5;34m0\u001B[0m │\n",
       "│ (\u001B[38;5;33mTextVectorization\u001B[0m)             │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_10 (\u001B[38;5;33mEmbedding\u001B[0m)        │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m500\u001B[0m, \u001B[38;5;34m128\u001B[0m)       │     \u001B[38;5;34m1,280,000\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ sinusoidal_positional_encoding… │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m500\u001B[0m, \u001B[38;5;34m128\u001B[0m)       │             \u001B[38;5;34m0\u001B[0m │\n",
       "│ (\u001B[38;5;33mSinusoidalPositionalEncoding\u001B[0m)  │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ encoder_layer_20 (\u001B[38;5;33mEncoderLayer\u001B[0m) │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m500\u001B[0m, \u001B[38;5;34m128\u001B[0m)       │       \u001B[38;5;34m198,272\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ encoder_layer_21 (\u001B[38;5;33mEncoderLayer\u001B[0m) │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m500\u001B[0m, \u001B[38;5;34m128\u001B[0m)       │       \u001B[38;5;34m198,272\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lambda_10 (\u001B[38;5;33mLambda\u001B[0m)              │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m)            │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_142 (\u001B[38;5;33mDense\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1\u001B[0m)              │           \u001B[38;5;34m129\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ text_vectorization_10           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TextVectorization</span>)             │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ sinusoidal_positional_encoding… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SinusoidalPositionalEncoding</span>)  │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ encoder_layer_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">EncoderLayer</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">198,272</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ encoder_layer_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">EncoderLayer</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">198,272</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lambda_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_142 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m1,676,673\u001B[0m (6.40 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,676,673</span> (6.40 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m1,676,673\u001B[0m (6.40 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,676,673</span> (6.40 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 118
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Datasets",
   "id": "22ede6d6eb082c47"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T05:40:03.248596Z",
     "start_time": "2025-02-23T05:40:02.621529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "raw_train_set, raw_valid_set, raw_test_set = tfds.load(\n",
    "    name=\"imdb_reviews\",\n",
    "    split=[\"train[:90%]\", \"train[90%:]\", \"test\"],\n",
    "    as_supervised=True\n",
    ")\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "train_set = raw_train_set.shuffle(5000, seed=42).batch(32).prefetch(1)\n",
    "valid_set = raw_valid_set.batch(32).prefetch(1)\n",
    "test_set = raw_test_set.batch(32).prefetch(1)"
   ],
   "id": "ce354330c5ef63aa",
   "outputs": [],
   "execution_count": 119
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training",
   "id": "a7ebab41f621698f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T05:40:07.030434Z",
     "start_time": "2025-02-23T05:40:03.254270Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# training dataset에서 텍스트만 추출하여 adapt 수행\n",
    "train_text = train_set.map(lambda text, label: text)\n",
    "text_vec_layer.adapt(train_text)"
   ],
   "id": "a41dd09f741dab36",
   "outputs": [],
   "execution_count": 120
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T05:40:10.193748Z",
     "start_time": "2025-02-23T05:40:10.178798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ],
   "id": "b00b4e7e480fb83a",
   "outputs": [],
   "execution_count": 121
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:03:47.156753Z",
     "start_time": "2025-02-23T05:40:18.928499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import datetime\n",
    "\n",
    "# 텐서보드 로그 디렉토리 설정\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "\n",
    "# 콜백 설정\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=log_dir,\n",
    "                                   histogram_freq=1,\n",
    "                                   write_graph=True,\n",
    "                                   update_freq='epoch')\n",
    "    ]\n",
    "\n",
    "history = model.fit(\n",
    "    train_set,\n",
    "    validation_data=valid_set,\n",
    "    epochs=30,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ],
   "id": "3997ac0abeabb083",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001B[1m704/704\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m154s\u001B[0m 214ms/step - accuracy: 0.4963 - loss: 0.7832 - val_accuracy: 0.5024 - val_loss: 0.6968\n",
      "Epoch 2/30\n",
      "\u001B[1m704/704\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m150s\u001B[0m 213ms/step - accuracy: 0.4965 - loss: 0.6968 - val_accuracy: 0.4976 - val_loss: 0.6963\n",
      "Epoch 3/30\n",
      "\u001B[1m704/704\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m153s\u001B[0m 217ms/step - accuracy: 0.5361 - loss: 0.6865 - val_accuracy: 0.6940 - val_loss: 0.6766\n",
      "Epoch 4/30\n",
      "\u001B[1m704/704\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m155s\u001B[0m 220ms/step - accuracy: 0.7058 - loss: 0.5755 - val_accuracy: 0.8132 - val_loss: 0.4289\n",
      "Epoch 5/30\n",
      "\u001B[1m704/704\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m157s\u001B[0m 223ms/step - accuracy: 0.8305 - loss: 0.3860 - val_accuracy: 0.8580 - val_loss: 0.3380\n",
      "Epoch 6/30\n",
      "\u001B[1m704/704\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m159s\u001B[0m 227ms/step - accuracy: 0.9041 - loss: 0.2581 - val_accuracy: 0.8696 - val_loss: 0.3365\n",
      "Epoch 7/30\n",
      "\u001B[1m704/704\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m160s\u001B[0m 227ms/step - accuracy: 0.9377 - loss: 0.1772 - val_accuracy: 0.8580 - val_loss: 0.3856\n",
      "Epoch 8/30\n",
      "\u001B[1m704/704\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m161s\u001B[0m 228ms/step - accuracy: 0.9651 - loss: 0.1157 - val_accuracy: 0.8488 - val_loss: 0.4283\n",
      "Epoch 9/30\n",
      "\u001B[1m704/704\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m159s\u001B[0m 226ms/step - accuracy: 0.9716 - loss: 0.0965 - val_accuracy: 0.8488 - val_loss: 0.5180\n"
     ]
    }
   ],
   "execution_count": 122
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:05:36.326183Z",
     "start_time": "2025-02-23T06:04:48.837584Z"
    }
   },
   "cell_type": "code",
   "source": "model.evaluate(test_set)",
   "id": "95f9d02d34d80ef9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m782/782\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m47s\u001B[0m 61ms/step - accuracy: 0.8602 - loss: 0.3635\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3617170751094818, 0.859000027179718]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 124
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
