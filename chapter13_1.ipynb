{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "2장에서는 판다스를 사용해서 **csv**파일을 로드 해왔고, 사이킷런의 여러가지 변환기를 사용해서 전처리를 수행했었다. 예를 들어서 SimpleImputer, StandardScaler, OneHotIncoder\n",
    "\n",
    "편리하긴 했지만, 대용량 데이터셋에서 텐서플로 모델을 훈련할 때는 텐서플로 자체의 데이터 로드 및 전처리 API인 `tf.data`를 사용하는게 좋다.\n",
    "\n",
    "매우 효율적으로 데이터를 로드하고 전처리 할 수 있다. 멀티쓰레드와 큐를 사용해서 여러 파일에서 동시에 읽고, 샘플을 셔플링 하거나 배치로 만드는 등의 작업을 할 수 있다.\n",
    "\n",
    "또 GPU가 현재 배치로 열심히 모델을 훈련시키는 동안 CPU는 다음 배치를 로드하고 전처리 하고 있을 수 있다.\n",
    "\n",
    "그리고 메모리보다 큰 데이터셋을 처리할 수 있고, 하드웨어 리소스를 최대한 활용할 수 있어서 훈련 속도가 향상된다. 기본 기능으로 csv파일, 고정 길이의 이진 레코드를 가진 이진 파일, 텐서플로의 TFRecord 포맷을 사용하는 이진파일에서 데이터를 읽을 수 있다. 이 포맷은 길이가 다른 이진 레코드를 지원함.\n",
    "\n",
    "\n",
    "케라스는 **모델에 포함**시킬 수 있는 강력하면서도 사용하기 쉬운 전처리 층을 제공한다. 제품 환경에 배포할 때 다른 전처리 코드 추가 없이 원시 데이터를 직접 주입할 수 있음.\n",
    "\n",
    "이 챕터에서는 먼저 tf.data API와 TFRecoed 포맷을 살펴본다. 그런 다음 케라스 전처리 층과 이를 tf.data API와 함꼐 사용하는 방법을 알아본다.\n",
    "\n",
    "마지막으로 데이터를 로드하고 전처리하는데 유용한 몇 가지 라이브러리를 간략하게 살펴본다."
   ],
   "id": "cdb19ad8c95623e2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 데이터 API",
   "id": "117d473acc637b78"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "데이터 API의 중심에는 tf.data.Dataset 개념이 있다. 이는 데이터 항목의 시퀀스는 나타낸다.\n",
    "\n",
    "tf.data.Data.from_tensor_slices()를 사용해서 간단한 텐서로 데이터셋을 생성해보겠다. **그러니까 텐서로 Dataset을 만들어주는 함수인 것이다.**\n",
    "\n",
    "텐서를 받아서 첫 번째 차원을 따라서 X의 각 원소가 아이템으로 표현되는 Dataset을 만든다. 즉 아래 코드에서는 10개의 아이템을 가지는 것이다."
   ],
   "id": "7f29dfb002f68178"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T18:38:20.290300Z",
     "start_time": "2025-02-08T18:38:17.439428Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from numpy import dtype"
   ],
   "id": "aa095ff8bfe2ba3a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/barrett11357/coding/pycharm/handson_ml_--/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T18:38:20.401939Z",
     "start_time": "2025-02-08T18:38:20.299402Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = tf.range(10) # 임의의 데이터 생성\n",
    "print(X)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "print(dataset) # 10개의 아이템을 가짐\n",
    "\n",
    "# for 문으로 순회할 수 있다.\n",
    "for x in dataset:\n",
    "    print(x)"
   ],
   "id": "8a7e2a8f8d13d3b8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6 7 8 9], shape=(10,), dtype=int32)\n",
      "<_TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-09 03:38:20.302018: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M4\n",
      "2025-02-09 03:38:20.302310: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-02-09 03:38:20.302320: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1739039900.302640 2553019 pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1739039900.302906 2553019 pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2025-02-09 03:38:20.399930: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Dataset에는 텐서 튜플, 이름/텐서 쌍의 딕셔너리, 심지어 중첨된 튜플과 딕셔너리도 포함될 수 있다.\n",
    "\n",
    "`from_tensor_slices()`는 중접 구조를 슬라이싱할 때, **데이터셋의 튜플/딕셔너리 구조를 유지하면서 그 안에 포함된 텐서만 슬라이싱한다.**\n",
    "\n"
   ],
   "id": "9793061350a9142"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T18:38:20.468862Z",
     "start_time": "2025-02-08T18:38:20.462309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 이렇게 복잡하게 충접된 딕셔너리도 슬라이싱해서 Dataset으로 바꿀 수 있다.\n",
    "\n",
    "X_nested = {\"a\":([1,2,3],[4,5,6]), \"b\":[7,8,9]} # 숫자가 모두 동일하게 3개가 아니게 되면 차원 에러가 발생한다.\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X_nested)\n",
    "for x in dataset:\n",
    "    print(x)"
   ],
   "id": "940622daab1853a9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': (<tf.Tensor: shape=(), dtype=int32, numpy=1>, <tf.Tensor: shape=(), dtype=int32, numpy=4>), 'b': <tf.Tensor: shape=(), dtype=int32, numpy=7>}\n",
      "{'a': (<tf.Tensor: shape=(), dtype=int32, numpy=2>, <tf.Tensor: shape=(), dtype=int32, numpy=5>), 'b': <tf.Tensor: shape=(), dtype=int32, numpy=8>}\n",
      "{'a': (<tf.Tensor: shape=(), dtype=int32, numpy=3>, <tf.Tensor: shape=(), dtype=int32, numpy=6>), 'b': <tf.Tensor: shape=(), dtype=int32, numpy=9>}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-09 03:38:20.467280: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 연쇄 변환",
   "id": "7bc4b6d363dedf7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "데이터셋이 준비되면 변환 메서드를 호출해서 여러 종류의 변환을 수행할 수 있다.\n",
    "**각 메서드는 새로운 데이터셋을 반환**해서 변환 메서드들을 체이닝 할 수 있다."
   ],
   "id": "78e8a60f9d2aef64"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T18:38:20.485625Z",
     "start_time": "2025-02-08T18:38:20.475840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(tf.range(10)) # tf.range(10)로 생성된건 Dataset이 아니라 텐서라서 repeat(), batch()를 쓸 수 없다.\n",
    "dataset = dataset.repeat(3).batch(7) # 3번 반복하고, 7개씩 그룹으로 묶어서 배치를 만든 것이다. 마지막 배치는 데이터가 2개 밖에 없음.\n",
    "for x in dataset:\n",
    "    print(x)"
   ],
   "id": "b4018af072906b19",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "`map()`을 호출해서 각 아이템들에 변형을 가할 수도 있다. 아래 코드는 모든 아이템에 2를 곱한 새로운 데이터셋을 만든다.",
   "id": "4f24133e51737a7c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T18:38:20.559005Z",
     "start_time": "2025-02-08T18:38:20.531173Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = dataset.map(lambda x: x * 2)\n",
    "for x in dataset:\n",
    "    print(x)"
   ],
   "id": "41cc0e8636df9356",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 0  2  4  6  8 10 12], shape=(7,), dtype=int32)\n",
      "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int32)\n",
      "tf.Tensor([16 18], shape=(2,), dtype=int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-09 03:38:20.557426: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "map()을 사용해서 데이터에 어떤 전처리 작업도 적용할 수 있다. **복잡한 계산을 포함하는 경우 여러 쓰레드로 나누어서 속도를 높이는 것이 좋다.**\n",
    "\n",
    "이를 위해서 num_parallel_calls 매개변수에 실행한 스레드 개수나, tf.data.AUTOTUNE을 지정할 수 있다. map()에 전달하는 메서드는 텐서플로 함수로 변환할 수 있어야 한다. <- 자세한 내용은 챕터12에 나옴.\n",
    "\n",
    "아래 코드를 실행해보면 여러 쓰레드를 사용해서 데이터를 병렬로 처리해서 아이템의 순서가 보장되지 않은 모습을 볼 수 있다. 파이썬에서 멀티쓰레드 다시 실습해보고, 자바에서도 다시 좀 해보면 좋을 듯."
   ],
   "id": "7c09e24a6483c11b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T18:38:20.580226Z",
     "start_time": "2025-02-08T18:38:20.568602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = dataset.map(lambda x: x * 2, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "for x in dataset:\n",
    "    print(x)"
   ],
   "id": "e124e581e5e984a6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 0  4  8 12 16 20 24], shape=(7,), dtype=int32)\n",
      "tf.Tensor([28 32 36  0  4  8 12], shape=(7,), dtype=int32)\n",
      "tf.Tensor([16 20 24 28 32 36  0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 4  8 12 16 20 24 28], shape=(7,), dtype=int32)\n",
      "tf.Tensor([32 36], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "또 `filter()`메서드를 사용해서 데이터셋을 필터링할 수도 있다.\n",
    "\n",
    "아래 코드는 아이템의 합이 25보다 큰 배치만 담은 데이터셋을 만든다."
   ],
   "id": "1ea42b5c9b7f5569"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T18:38:20.608542Z",
     "start_time": "2025-02-08T18:38:20.589420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(tf.range(10)).repeat(3).batch(7)\n",
    "\n",
    "dataset = dataset.filter(lambda x : tf.reduce_sum(x) > 25)\n",
    "\n",
    "for x in dataset:\n",
    "    print(x)"
   ],
   "id": "611286e601f9761c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "데이터셋에 있는 몇 개의 아이템만 보고 싶을 떄는 `take()`메서드를 사용한다.",
   "id": "de308ec73a7224e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T18:38:20.630451Z",
     "start_time": "2025-02-08T18:38:20.620853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for x in dataset.take(2): # 2개의 아이템, 즉 2개의 배치만 나온다.\n",
    "    print(x)"
   ],
   "id": "63e19e1899e52459",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 데이터 셔플링",
   "id": "49b76ef6d200e5d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "4장에서 배웠는데, 경사하강법은 훈련 세트에 있는 샘플이 **독립적**이고 **동일한 분포**일 때 최고의 성능을 발휘한다.\n",
    "\n",
    "데이터를 섞는 간단한 방법은 `shuffle()`메서드를 사용하는 방법이다.\n",
    "이 메서는 원본 데이터셋의 처음 아이템을 buffer_size개수만큼 추출해서 버퍼에 채운다. 그 다음 새로운 아이템이 요청되면 이 버퍼에서 랜덤하게 하나를 꺼내서 반환한다.\n",
    "그리고 원본 데이터셋에서 새로운 아이템을 하나 꺼내서 버퍼를 채운다. 버퍼가 텅 빌 때까지 이 과정을 계속 반복하는 것이다.\n",
    "\n",
    "이 메서드를 사용하려면 버퍼 크기를 지정해야 하는데, 원리만 봐도 알겠듯이 버퍼 크기를 충분히 크게 하는 것이 중요하다."
   ],
   "id": "5fbde42d7601af7e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T18:38:20.671968Z",
     "start_time": "2025-02-08T18:38:20.646323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = tf.data.Dataset.range(10).repeat(2)\n",
    "dataset = dataset.shuffle(buffer_size=4, seed=42).batch(7)\n",
    "\n",
    "for x in dataset:\n",
    "    print(x)"
   ],
   "id": "940b2ac9245b5b1e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 4 2 3 5 0 6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([9 8 2 0 3 1 4], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 7 9 6 7 8], shape=(6,), dtype=int64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-09 03:38:20.670603: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 메모리 용량보다 큰 대규모 데이터셋에는 어떻게 해야할까?",
   "id": "3192f1992a6f3250"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "메모리 용량보다 큰 대규모 데이터셋의 경우 버퍼가 데이터셋에 비해 작을 수 밖에 없기 때문에 이 방법 만으로는 충분하지 않다.\n",
    "\n",
    "이를 해결하는 방법은 원본 데이터 자체를 섞는 것이다. (리눅스에서는 shuffle())\n",
    "\n",
    "이렇게 하면 셔플링 효과가 크게 향상\n",
    "\n",
    "그런데 원본 데이터가 섞여 있어도 일반적으로 에포크마다 한 번씩 더 섞어야 한다.\n",
    "그렇지 않으면 에포크마다 동일한 순서가 반복되어서 모델에 편향이 추가된다. 원본 데이터에 존재하는 가짜 패턴 때문이다.\n",
    "\n",
    "더 섞기 위해서 사용하는 방법으로 원본 데이터를 여러 파일로 나눈 다음에 훈련하는 동안 램덤으로 몇 개를 골라서 읽는 것이다.\n",
    "\n",
    "그런데 아직도 문제가 있는게, 동일한 파일에 있는 샘플들을 여전히 같은 순서로 읽힌다.\n",
    "\n",
    "이를 피하기 위해서 파일 여러 개를 랜덤으로 선택하고, 파일에서 동시에 읽은 레코드(행, 튜플)를 돌아가면서 반환할 수 있다.\n",
    "그리고 또 여기에 shuffle()메서드를 사용해서 그 위에 셔플링 버퍼를 추가할 수 있다.\n",
    "\n",
    "이제 직접 해보자!!!\n",
    "\n",
    "\n"
   ],
   "id": "94d88393bbbaf6db"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 여러 파일에서 한 줄씩 번갈아 가면서 읽기",
   "id": "9d9b7dba194eb472"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T18:38:20.773157Z",
     "start_time": "2025-02-08T18:38:20.677932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 추가 코드 - 캘리포니아 주택 데이터셋을 가져오고, 분할하고, 정규화합니다.\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)"
   ],
   "id": "2a64b4a8be0a1f18",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T18:38:20.832810Z",
     "start_time": "2025-02-08T18:38:20.781651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 추가 코드 - 데이터셋을 20개 파일로 분할하여 CSV 파일로 저장합니다.\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def save_to_csv_files(data, name_prefix, header=None, n_parts=10):\n",
    "    housing_dir = Path() / \"datasets\" / \"housing\"\n",
    "    housing_dir.mkdir(parents=True, exist_ok=True)\n",
    "    filename_format = \"my_{}_{:02d}.csv\"\n",
    "\n",
    "    filepaths = []\n",
    "    m = len(data)\n",
    "    chunks = np.array_split(np.arange(m), n_parts)\n",
    "    for file_idx, row_indices in enumerate(chunks):\n",
    "        part_csv = housing_dir / filename_format.format(name_prefix, file_idx)\n",
    "        filepaths.append(str(part_csv))\n",
    "        with open(part_csv, \"w\") as f:\n",
    "            if header is not None:\n",
    "                f.write(header)\n",
    "                f.write(\"\\n\")\n",
    "            for row_idx in row_indices:\n",
    "                f.write(\",\".join([repr(col) for col in data[row_idx]]))\n",
    "                f.write(\"\\n\")\n",
    "    return filepaths\n",
    "\n",
    "train_data = np.c_[X_train, y_train]\n",
    "valid_data = np.c_[X_valid, y_valid]\n",
    "test_data = np.c_[X_test, y_test]\n",
    "header_cols = housing.feature_names + [\"MedianHouseValue\"]\n",
    "header = \",\".join(header_cols)\n",
    "\n",
    "# 분할된 csv 파일들의 경로는 담은 변수들\n",
    "train_filepaths = save_to_csv_files(train_data, \"train\", header, n_parts=20)\n",
    "valid_filepaths = save_to_csv_files(valid_data, \"valid\", header, n_parts=10)\n",
    "test_filepaths = save_to_csv_files(test_data, \"test\", header, n_parts=10)\n",
    "\n",
    "# 꼭 경로들을 하나하나 리스트에 담지 않고, 파일 패턴을 사용할 수도 있다고 한다."
   ],
   "id": "42beaedef9e7d52d",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T18:38:20.844552Z",
     "start_time": "2025-02-08T18:38:20.842144Z"
    }
   },
   "cell_type": "code",
   "source": "print(train_filepaths)",
   "id": "25ad0c5ac797f2f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['datasets/housing/my_train_00.csv', 'datasets/housing/my_train_01.csv', 'datasets/housing/my_train_02.csv', 'datasets/housing/my_train_03.csv', 'datasets/housing/my_train_04.csv', 'datasets/housing/my_train_05.csv', 'datasets/housing/my_train_06.csv', 'datasets/housing/my_train_07.csv', 'datasets/housing/my_train_08.csv', 'datasets/housing/my_train_09.csv', 'datasets/housing/my_train_10.csv', 'datasets/housing/my_train_11.csv', 'datasets/housing/my_train_12.csv', 'datasets/housing/my_train_13.csv', 'datasets/housing/my_train_14.csv', 'datasets/housing/my_train_15.csv', 'datasets/housing/my_train_16.csv', 'datasets/housing/my_train_17.csv', 'datasets/housing/my_train_18.csv', 'datasets/housing/my_train_19.csv']\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T18:38:20.863718Z",
     "start_time": "2025-02-08T18:38:20.861459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 파일 하나를 열어서 몇 줄만 읽어본다.\n",
    "print(\"\".join(open(train_filepaths[0]).readlines()[:4]))"
   ],
   "id": "71d989a9e5672522",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n",
      "3.5214,15.0,3.0499445061043287,1.106548279689234,1447.0,1.6059933407325193,37.63,-122.43,1.442\n",
      "5.3275,5.0,6.490059642147117,0.9910536779324056,3464.0,3.4433399602385686,33.69,-117.39,1.687\n",
      "3.1,29.0,7.5423728813559325,1.5915254237288134,1328.0,2.2508474576271187,38.44,-122.98,1.621\n",
      "\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "이제 이런 **파일 경로를 담은 데이터셋**을 만든다. `tf.data.Dataset.list_files()`가 파일 경로를 섞어서 반환한다.",
   "id": "8dedfb1d2921be1a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T18:38:20.903550Z",
     "start_time": "2025-02-08T18:38:20.888013Z"
    }
   },
   "cell_type": "code",
   "source": [
    "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42) # 파일 경로를 담은 데이터셋을 반환한다. 파일 경로들을 섞어준다.\n",
    "for x in filepath_dataset:\n",
    "    print(x)"
   ],
   "id": "e7e951a7c2b2f943",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'datasets/housing/my_train_05.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_16.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_01.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_17.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_00.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_14.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_10.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_02.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_12.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_19.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_07.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_09.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_13.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_15.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_11.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_18.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_04.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_06.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_03.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets/housing/my_train_08.csv', shape=(), dtype=string)\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "이제 `interleave()`메서드를 호출해서 한 번에 다섯 개의 파일을 한 줄씩 번갈아 읽는다.\n",
    "\n",
    "이 메서드는 filepath_dataset에 있는 다섯 개의 파일 경로에서 데이터를 데이터를 읽는 데이터셋을 만든다.\n",
    "\n",
    "전달한 함수를 각 파일에 대해 호출해서 새로운 데이터셋을 만드는 것이다."
   ],
   "id": "a6681da92972cd5d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. **전체 구조**\n",
    "   - `filepath_dataset`에서 파일 경로들을 가져와서\n",
    "   - 각 파일을 `TextLineDataset`으로 변환하여 읽습니다\n",
    "   - 5개의 파일을 동시에 번갈아가며 읽습니다\n",
    "\n",
    "2. **interleave 함수의 역할**\n",
    "   - 첫 번째 인자 (lambda 함수):\n",
    "     * 각 파일 경로를 받아서\n",
    "     * `TextLineDataset`을 생성\n",
    "     * `skip(1)`로 첫 줄(헤더)를 건너뜀\n",
    "\n",
    "3. **매개변수 설명**\n",
    "   - `cycle_length=5`:\n",
    "     * 5개의 파일을 동시에 열어서\n",
    "     * 번갈아가며 한 줄씩 읽음\n",
    "   - `num_parallel_calls=tf.data.AUTOTUNE`:\n",
    "     * 멀티스레드로 파일 읽기 수행\n",
    "     * 시스템이 자동으로 최적의 스레드 수 결정\n",
    "\n",
    "4. **데이터 읽기 순서**\n",
    "   ```\n",
    "   파일1의 첫째줄 → 파일2의 첫째줄 → 파일3의 첫째줄 → 파일4의 첫째줄 → 파일5의 첫째줄\n",
    "   파일1의 둘째줄 → 파일2의 둘째줄 → ... (반복)\n",
    "   ```"
   ],
   "id": "955010a7311d44eb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T18:38:20.932441Z",
     "start_time": "2025-02-08T18:38:20.914783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_readers = 5 # 동시에 5개의 파일을 읽겠다는 뜻\n",
    "dataset  = filepath_dataset.interleave(\n",
    "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1), # 각 파일을 한 줄씩 읽는 함수. slip(1)은 열 이름은 건너뛰기 위해 넣음.\n",
    "    cycle_length=n_readers,\n",
    "    num_parallel_calls=tf.data.AUTOTUNE) # 멀티쓰레드로 수행"
   ],
   "id": "f9823d93b4f0d24f",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T18:38:20.956296Z",
     "start_time": "2025-02-08T18:38:20.942008Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for x in dataset.take(5): # dataset.take(1)은 하나의 아이템만 포함하는 \"새로운 데이터셋\"을 반환하는 것이다. 그래서 이렇게 순회를 돌리는 것이다.\n",
    "    print(x)"
   ],
   "id": "d5c62d1bb851bd73",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'4.5909,16.0,5.475877192982456,1.0964912280701755,1357.0,2.9758771929824563,33.63,-117.71,2.418', shape=(), dtype=string)\n",
      "tf.Tensor(b'2.4792,24.0,3.4547038327526134,1.1341463414634145,2251.0,3.921602787456446,34.18,-118.38,2.0', shape=(), dtype=string)\n",
      "tf.Tensor(b'4.2708,45.0,5.121387283236994,0.953757225433526,492.0,2.8439306358381504,37.48,-122.19,2.67', shape=(), dtype=string)\n",
      "tf.Tensor(b'2.1856,41.0,3.7189873417721517,1.0658227848101265,803.0,2.0329113924050635,32.76,-117.12,1.205', shape=(), dtype=string)\n",
      "tf.Tensor(b'4.1812,52.0,5.701388888888889,0.9965277777777778,692.0,2.4027777777777777,33.73,-118.31,3.215', shape=(), dtype=string)\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 데이터 전처리",
   "id": "3ed7737c8d8ba639"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "앞에서 만든 dataset은 각 샘플이 바이트 문자열이 담긴 텐서로 저장되어 있는 것이다.\n",
    "\n",
    "이제 **문자열을 파싱**하고, 데이터 스케일을 조정하는 등 약간의 전처리가 필요하다.\n",
    "\n",
    "이런 전처리를 수행하기 위해 사용자 정의 함수를 몇 개 만들어 본다."
   ],
   "id": "89df02ea3a293af9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T18:38:20.973319Z",
     "start_time": "2025-02-08T18:38:20.967610Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 추가 코드 - 각 특성의 평균 및 표준 편차 계산\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)"
   ],
   "id": "60ac56925afc9e77",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler()"
      ],
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StandardScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>StandardScaler</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.preprocessing.StandardScaler.html\">?<span>Documentation for StandardScaler</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>StandardScaler()</pre></div> </div></div></div></div>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "StandardScaler와 동일한 기능을 하는 코드를 작성한다.",
   "id": "84db28b44f43d668"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T18:38:21.182382Z",
     "start_time": "2025-02-08T18:38:21.179324Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_mean, X_std = scaler.mean_, scaler.scale_  # 각 특성의 평균과 표준편차를 담은 1D 텐서\n",
    "n_inputs = 8 # 데이터의 특성 수\n",
    "\n",
    "# csv 문자열 한 줄을 받아서 파싱을 해서 텐서로 반환하는 함수\n",
    "def parse_csv_line(line) :\n",
    "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)] # 파싱 할 때 누락된 값을 위한 기본값 설정\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    return tf.stack(fields[:-1]), tf.stack(fields[-1:]) # 데이터와 레이블을 각각 반환하는 것이다.\n",
    "\n",
    "# 평균과 표준편자를 이용해서 각 샘플을 표준화 하는 함수. sklearn의 StandradScaler와 똑같은 기능을 한다.\n",
    "def preprocess_csv_line(line):\n",
    "    x, y = parse_csv_line(line)\n",
    "    return (x - X_mean) / X_std, y"
   ],
   "id": "a6d39a502d9f461a",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T18:38:21.330510Z",
     "start_time": "2025-02-08T18:38:21.281377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 테스트해보면 잘 작동한다.\n",
    "preprocess_csv_line(b'4.2083,44.0,5.323204419889502,0.9171270718232044,846.0,2.3370165745856353,37.47,-122.2,2.782')"
   ],
   "id": "a83f093c95d7b746",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
       " array([ 0.16579159,  1.216324  , -0.05204396, -0.39210168, -0.5277444 ,\n",
       "        -0.26334172,  0.8543046 , -1.3072058 ], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.782], dtype=float32)>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 데이터 적재와 전처리 합치기",
   "id": "aae9965c10fc1e6d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "좋아. 이제 앞에서 해본 것들을 전부 다 합친 하나의 헬퍼 함수를 만들어본다.\n",
    "\n",
    "1. 파일 경로가 담긴 리스트가 들어오면, 이걸 다 섞으면서 파일 경로 데이터셋에 담는다.\n",
    "2. 파일 경로 데이터 셋에서 n개의 파일 경로만 랜덤으로 뽑아서 번갈아가면서 한 줄씩 읽어와서 dataset에 저장한다.\n",
    "3. dataset에 저장된 csv 문자열들을 파싱해서 숫자로 만들고, 표준화를 가한다.\n",
    "4. 배치로 쪼개서 반환한다."
   ],
   "id": "c7e040c60c71c9bc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T18:38:21.361244Z",
     "start_time": "2025-02-08T18:38:21.356673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def csv_reader_dataset(filepaths, seed=42, n_readers=5, n_inputs = 8, buffer_size=10000, batch_size=32):\n",
    "    filepath_dataset = tf.data.Dataset.list_files(filepaths, seed=seed)\n",
    "\n",
    "    dataset = filepath_dataset.interleave(\n",
    "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "        cycle_length=n_readers,\n",
    "        num_parallel_calls=tf.data.AUTOTUNE) # 멀티 스레드 사용\n",
    "\n",
    "    dataset = dataset.map(preprocess_csv_line, num_parallel_calls=tf.data.AUTOTUNE) # 멀티스레드 사용\n",
    "\n",
    "    dataset = dataset.shuffle(buffer_size=buffer_size, seed=seed)\n",
    "\n",
    "    return dataset.batch(batch_size).prefetch(1)"
   ],
   "id": "f87d01fcd95b6593",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "마지막에 `prefetch(1)`을 호출하면 데이터셋은 항상 한 배치가 미리 준비되도록 최선을 다한다. 훈련 알고리즘이 한 배치로 작업을 하는 동안 이 데이터셋은 동시에 다음 배치를 준비한다.\n",
    "\n",
    "매개변수로 `tf.data.AUTOTUNE을 전달하면 텐서플로가 자동으로 얼마 만큼의 배치를 준비해야 하는지 결정한다."
   ],
   "id": "14c71ff55e4206fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T18:38:21.793172Z",
     "start_time": "2025-02-08T18:38:21.413516Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = csv_reader_dataset(train_filepaths, n_readers=5, buffer_size=10000, batch_size=32) # 변환\n",
    "\n",
    "# 한 배치의 데이터와 레이블을 각각 구경해본다.\n",
    "for line in dataset.take(1):\n",
    "    x,y  = line\n",
    "    print(x)\n",
    "\n",
    "for line in dataset.take(1):\n",
    "    x,y  = line\n",
    "    print(y)"
   ],
   "id": "32952ce285023e9b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-1.39574516e+00 -4.94068526e-02 -2.28308082e-01  2.26482734e-01\n",
      "   2.25936222e+00  3.52006316e-01  9.66738582e-01 -1.41216016e+00]\n",
      " [ 2.71126270e+00 -1.07781315e+00  6.94131434e-01 -1.48705527e-01\n",
      "   5.18105030e-01  3.50729406e-01 -8.22851539e-01  8.06805968e-01]\n",
      " [-1.34846434e-01 -1.86889505e+00  1.03250686e-02 -1.37871787e-01\n",
      "  -1.28934488e-01  3.14351842e-02  2.68705696e-01  1.32121444e-01]\n",
      " [ 9.03177410e-02  9.78999496e-01  1.32758200e-01 -1.37537822e-01\n",
      "  -2.33884469e-01  1.02115452e-01  9.76108432e-01 -1.41216016e+00]\n",
      " [ 5.21880873e-02 -2.02711129e+00  2.94010907e-01 -2.40344461e-02\n",
      "   1.62187666e-01 -2.84451842e-02  1.41179419e+00 -9.37379360e-01]\n",
      " [-6.72276020e-01  2.97013279e-02 -7.69225836e-01 -1.50867864e-01\n",
      "   4.96202409e-01 -2.74199769e-02 -7.85372376e-01  7.71822453e-01]\n",
      " [-8.11177075e-01  3.46134037e-01 -2.18263835e-01 -8.01026970e-02\n",
      "   6.63637593e-02  2.67242640e-01  1.93749100e-01  3.02040339e-01]\n",
      " [-6.89402997e-01  1.84918952e+00 -8.05119038e-01 -8.77811536e-02\n",
      "   1.09031057e+00 -3.60031277e-01  9.94848013e-01 -1.41715515e+00]\n",
      " [-1.10259688e+00 -2.86731392e-01 -1.48109841e+00 -7.15598762e-02\n",
      "   1.25184226e+00  2.57247835e-01 -7.47894943e-01  6.56878173e-01]\n",
      " [ 4.09234941e-01  5.83458602e-01 -2.00159073e-01 -2.45306104e-01\n",
      "  -4.70250100e-01  1.46773413e-01 -8.13481748e-01  7.61828780e-01]\n",
      " [ 2.95788646e-01 -1.86889505e+00 -3.79276693e-01  1.76563695e-01\n",
      "  -5.14055312e-01 -4.24348533e-01  7.51238644e-01 -1.11229706e+00]\n",
      " [-1.26459575e+00 -1.28515035e-01 -8.81155133e-01 -1.01157047e-01\n",
      "   4.37795460e-01 -6.79842848e-03 -7.01045990e-01  1.92094848e-01]\n",
      " [ 5.28075993e-01 -9.19596851e-01  4.10044730e-01  6.45770207e-02\n",
      "   4.04028922e-01  9.61895734e-02 -4.62122411e-01  6.81864262e-01]\n",
      " [-3.77923161e-01 -2.86731392e-01 -4.46743548e-01 -2.45428272e-02\n",
      "   1.00817585e+00 -3.70102614e-01  7.93401837e-01 -1.18226409e+00]\n",
      " [ 4.47151327e+00  4.25242215e-01  1.65102041e+00  4.77572083e-02\n",
      "  -5.60598373e-01 -2.35627834e-02  9.05837595e-01 -1.39216900e+00]\n",
      " [-7.89493501e-01 -1.55246222e+00 -5.15803218e-01 -1.86118677e-01\n",
      "   4.60610658e-01 -1.90741777e-01  1.41179419e+00 -8.92402172e-01]\n",
      " [-6.67824149e-01  1.05810761e+00 -5.46949089e-01 -5.04690468e-01\n",
      "  -1.39885783e-01  1.74680054e-01  1.09791398e+00 -1.37717283e+00]\n",
      " [-1.07965624e+00 -3.65839571e-01 -1.46916950e+00  7.01248869e-02\n",
      "  -6.14442229e-01  4.29697126e-01 -7.47894943e-01  6.56878173e-01]\n",
      " [ 1.20822990e+00 -9.19596851e-01  5.18467486e-01 -1.66410193e-01\n",
      "   8.73537511e-02  4.47405539e-02 -7.52578974e-01  1.17163742e+00]\n",
      " [ 1.06959081e+00 -1.07781315e+00  3.54996949e-01 -3.70874852e-01\n",
      "  -4.39221412e-01  2.04430923e-01 -1.00087416e+00  9.71729934e-01]\n",
      " [ 1.15663970e+00  2.97013279e-02  6.14650607e-01 -1.38230085e-01\n",
      "  -8.67234826e-01 -7.22591728e-02 -8.50959122e-01  8.11804712e-01]\n",
      " [-1.32247102e+00  1.84918952e+00 -1.09093559e+00 -2.19220463e-02\n",
      "   8.27907100e-02 -5.07874370e-01  1.00421774e+00 -1.41216016e+00]\n",
      " [-7.87136495e-01 -1.78978682e+00  3.37470472e-01  4.19017762e-01\n",
      "  -3.94503593e-01  6.38826489e-02  2.07802907e-01  7.17972592e-03]\n",
      " [ 2.85837203e-01 -3.65839571e-01  1.31497100e-01 -8.55614431e-03\n",
      "  -7.29430914e-01 -1.02260239e-01 -8.22851539e-01  8.11804712e-01]\n",
      " [-6.96211874e-01 -8.40488672e-01 -7.77298510e-01 -1.31911322e-01\n",
      "  -5.87063968e-01 -2.80263066e-01 -8.60328972e-01  8.36790740e-01]\n",
      " [-1.48986459e+00 -1.07781315e+00 -1.44267336e-01  2.65655890e-02\n",
      "  -3.76736224e-02  3.08774173e-01  5.02945244e-01 -1.07768334e-01]\n",
      " [-8.66433740e-01 -3.65839571e-01 -4.21191901e-01 -9.82737467e-02\n",
      "   5.29968917e-01  1.80531815e-02 -1.36628723e+00  1.24160457e+00]\n",
      " [-1.29481661e+00 -8.40488672e-01 -3.45116675e-01 -1.61973849e-01\n",
      "  -8.59021366e-01 -1.33793764e-02 -4.99599814e-01  1.09167290e+00]\n",
      " [-1.61797607e+00 -4.44947749e-01 -3.39971520e-02  1.09857887e-01\n",
      "  -3.37921858e-01  1.20357744e-01  1.01358759e+00 -1.35218680e+00]\n",
      " [-4.83408332e-01  1.84918952e+00 -4.80472982e-01 -1.14994191e-01\n",
      "  -5.05841851e-01 -1.91270337e-01  9.94848013e-01 -1.34718800e+00]\n",
      " [-1.09657359e+00  4.25242215e-01 -5.28250933e-01 -1.36215523e-01\n",
      "  -9.46631789e-01 -6.54358044e-02 -7.38525152e-01  1.14164889e+00]\n",
      " [-1.15088749e+00 -1.39424586e+00  6.24069834e+00  7.58018160e+00\n",
      "  -1.23684132e+00 -5.73484600e-01  2.12856674e+00 -7.47469306e-01]], shape=(32, 8), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1.316  ]\n",
      " [2.135  ]\n",
      " [1.565  ]\n",
      " [2.317  ]\n",
      " [1.185  ]\n",
      " [4.045  ]\n",
      " [0.71   ]\n",
      " [2.935  ]\n",
      " [1.621  ]\n",
      " [1.885  ]\n",
      " [3.133  ]\n",
      " [4.658  ]\n",
      " [3.331  ]\n",
      " [1.63   ]\n",
      " [1.693  ]\n",
      " [0.877  ]\n",
      " [1.335  ]\n",
      " [2.672  ]\n",
      " [0.555  ]\n",
      " [3.379  ]\n",
      " [2.905  ]\n",
      " [2.722  ]\n",
      " [2.043  ]\n",
      " [3.412  ]\n",
      " [5.00001]\n",
      " [5.00001]\n",
      " [3.701  ]\n",
      " [5.00001]\n",
      " [2.722  ]\n",
      " [4.333  ]\n",
      " [1.468  ]\n",
      " [5.00001]], shape=(32, 1), dtype=float32)\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 케라스와 Dataset 사용하기",
   "id": "3e239bcd971f235b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "앞에서 만든 csv_reader_dataset() 을 사용해서 훈련, 검증, 테스트 데이터를 가공해보자.",
   "id": "cb325153d1126c1b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T18:38:21.866497Z",
     "start_time": "2025-02-08T18:38:21.797687Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_set = csv_reader_dataset(train_filepaths, n_readers=5, buffer_size=10000, batch_size=32)\n",
    "valid_set = csv_reader_dataset(valid_filepaths, n_readers=5, buffer_size=10000, batch_size=32)\n",
    "test_set = csv_reader_dataset(test_filepaths, n_readers=5, buffer_size=10000, batch_size=32)"
   ],
   "id": "2df9e5575bb93c9b",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "그리고 이제 얘네를 가지고 훈련할 수 있다.\n",
    "\n",
    "그런데 앞에서 했던 것처럼 X_train, y_train이렇게 따로 따로 전달하는게 아니고, 현재 train_set, valid_set, test_set에는 다 레이블이 포함되어 있기 때문에 그냥 넣어주면 된다.\n",
    "\n",
    "데이터셋은 매 에포크마다 셔플된다. fit()메서드가 에포크마다 랜덤한 순서로 훈련 데이터셋을 한 번씩 반복한다."
   ],
   "id": "730072adf1a10829"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T18:38:23.452402Z",
     "start_time": "2025-02-08T18:38:21.880173Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"sgd\")\n",
    "model.fit(train_set, validation_data = valid_set, epochs = 1, verbose=2)"
   ],
   "id": "1ab4bd2b886c6c8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-09 03:38:22.039669: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n",
      "2025-02-09 03:38:23.108991: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 2454283735648041019\n",
      "2025-02-09 03:38:23.109003: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 4070741231059566193\n",
      "2025-02-09 03:38:23.109006: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 1524956519418951581\n",
      "2025-02-09 03:38:23.109011: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 10413961157246517750\n",
      "2025-02-09 03:38:23.109013: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 5854288071903206488\n",
      "2025-02-09 03:38:23.109017: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 2424753409805737674\n",
      "/Users/barrett11357/coding/pycharm/handson_ml_--/.venv/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 - 2s - 4ms/step - loss: 1.1600 - val_loss: 16.2034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-09 03:38:23.447173: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_2]]\n",
      "2025-02-09 03:38:23.447187: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 9766402229857520293\n",
      "2025-02-09 03:38:23.447195: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 14291216707504950871\n",
      "2025-02-09 03:38:23.447201: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 15065178145089487566\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x311a180d0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "이렇게 tf.data API를 사용해서 강력한 입력 파이프라인을 만드는 방법을 배웠다. 지금까지 사용한 csv 파일은 간단하고 편리해서 널리 통용되지만 효율적이지 않고, 대규모의 복잡한 (이미지, 오디오) 데이터 구조를 지원하지 못한다. 이 대신 TFRecord를 사용하는 방법을 알아본다.\n",
    "\n",
    "훈련과정에서 데이터를 적재하고 전처리하는 데 병목이 생기는 경우 유용하다."
   ],
   "id": "a3b96946429e48c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# TFRecord 포맷",
   "id": "87d0c7000586fa9e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "일단 코드는 건너 뛰고, 간단하게 설명만 남겨놓겠다.\n",
    "\n",
    "TFRecord는 직렬화된 **프로토콜 버퍼**를 담고 있다. 이건 확장성이 좋고 효율적인 이진 **포맷**이다.\n",
    "\n",
    "책에서 프로토콜 버퍼를 직접 정의하는 코드가 간단하게 나오는데, 일반적으로 텐서플로에서 사용할 프로토콜 버퍼 정의는 이미 컴파일 되어서 텐서플로 안에 파이썬 클래스로 포함되어 있다고 한다.\n",
    "그래서 그냥 프로토콜 버퍼 정의에 따라 생성된 파이썬 클래스를 다룰 줄만 알면 됨.\n",
    "\n",
    "그리고 그 파이썬 클래스를 직렬화해서 TFRecord에 저장하고, 이걸 다시 파싱해서 읽어내는 방식이다. 프로토콜 버퍼 정의만 제공하면 tf.io.decode_proto()함수를 사용해서 어떤 프로토콜 버퍼도 파싱할 수 있다.\n",
    "\n",
    "하지만 일반적으로 텐서플로가 제공하는 전용 파싱 연산을 제공하는 사전 정의된 프로토콜 버퍼를 대신 사용하는 것이 좋다.\n",
    "주요 프로토콜 버퍼로는 Example, SequenceExample이 있다.\n",
    "\n",
    "Example 객체는 하나의 Features객체를 가진다. Features객체는 특성 이름과 Feature객체를 매핑한 딕셔너리를 가지고 있다. Feature객체는 Bytelist, Floatlist, Int64List중 하나를 담고 있다.\n",
    "\n",
    "SequenceExample은 하나의 Features객체와 하나의 FeatureLists객체를 가지고 있다. FeatureLists객체는 이름과 FeatureList객체로 매핑된 딕셔너리를 가지고 있다. 그리고 FeatureList는 하나 이상의 Feature객체를 가지고 있다."
   ],
   "id": "d3c92c091bd667c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 케라스의 전처리",
   "id": "88698a3aac6ed1d1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "신경망에 사용할 데이터를 준비하려면 일반적으로 수치 특성 정규화, 범주형 특성이나 텍스트 인코딩, 이미지 자르기와 크기 조정 등의 작업이 필요하다.\n",
    "\n",
    "케라스는 모델에 포함할 수 있는 다양한 전처리 층을 제공한다."
   ],
   "id": "dc5ec3954b227a5a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Nomalization 층",
   "id": "dce143bcbdecfc10"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "이건 앞에서도 써봤지만 **입력 특성을 표준화할 수 있는** Nomalization층을 제공한다. 이 층을 만들 때 각 특성의 평균과 분산을 지정할 수도 있고, 아니면 fit()을 호출해서 모델을 훈련하기 전에 adapt()에 훈련 세트를 전달해서 특성의 평균과 분산을 계산할 수 있다.",
   "id": "39c4f1e2c3476046"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 전처리 불일치 문제를 해결하는 방법",
   "id": "b50447ad2983ff51"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T18:38:25.478325Z",
     "start_time": "2025-02-08T18:38:23.464606Z"
    }
   },
   "cell_type": "code",
   "source": [
    "norm_layer = tf.keras.layers.Normalization()\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    norm_layer,\n",
    "    tf.keras.layers.Dense(1, activation=\"relu\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"sgd\")\n",
    "\n",
    "# norm layer를 통과시키기 위해 특성만 뽑아냄. 현재 train_set에는 특성과 레이블이 같이 있는 상태다.\n",
    "# train_set은 이미 정규화가 된 상태이지만, 예시를 위해 한 번 더 정규화 한다ㅋㅋ\n",
    "X_train = train_set.unbatch().map(lambda x, y: x)\n",
    "norm_layer.adapt(X_train)\n",
    "\n",
    "model.fit(train_set, validation_data = valid_set, epochs=1)"
   ],
   "id": "d044031b24c4ae36",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    363/Unknown \u001B[1m1s\u001B[0m 3ms/step - loss: 2.7217"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-09 03:38:25.138502: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 8228886785398399008\n",
      "2025-02-09 03:38:25.138547: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 17957140967288023351\n",
      "2025-02-09 03:38:25.138555: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 3076765436023824323\n",
      "2025-02-09 03:38:25.138560: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 15456812406770037240\n",
      "2025-02-09 03:38:25.138562: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 18381312475554204736\n",
      "2025-02-09 03:38:25.138565: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 9752210912658541858\n",
      "2025-02-09 03:38:25.138567: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 206466625880080012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m363/363\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 4ms/step - loss: 2.7188 - val_loss: 0.5959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-09 03:38:25.473200: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 9878487928683836563\n",
      "2025-02-09 03:38:25.473214: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 1632688311992821238\n",
      "2025-02-09 03:38:25.473226: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 8728971235411287552\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x3130e5d90>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "모델에 Nomalization 전처리 층을 포함시켰기 때문에 정규화에 대해 신경 쓰지 않고 이 모델을 제품에 배포할 수 있다. 원시 데이터를 바로 주입해도 됨.\n",
    "\n",
    "**전처리 불일치 위험을 완전히 제거**한다.\n",
    "\n",
    "이렇게 모델에 전처리 층을 포함시키는 것은 좋은 생각이고 간단하지만, **훈련속도를 느리게 만든다**\n",
    "\n",
    "전처리가 훈련되는 동안 즉시 적용되기 때문에 에포크마다 매번 수행된다.\n",
    "\n",
    "이보다는 **전체 훈련 세트를 딱 한 번 전처리 하는 것이 낫다.**\n",
    "\n",
    "Nomalization층을 **독립적**으로 사용할 수 있다. <- 이건 뭔 얘기인가 했는데 Nomalization층을 모델 밖으로 빼서, 모델에 넣기 전에 전처리를 한다는 것임. 이런면 또 전처리 불일치 위험이 발생함."
   ],
   "id": "8b41bc9158f9dfbb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T18:38:25.754169Z",
     "start_time": "2025-02-08T18:38:25.489635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "norm_layer = tf.keras.layers.Normalization()\n",
    "\n",
    "adapt_ds = train_set.unbatch().map(lambda features, label: features) # 배치를 풀고 특성만 뽑아내서 정규화 층을 적응시킨다.\n",
    "norm_layer.adapt(adapt_ds)\n",
    "\n",
    "'''\n",
    "아래 코드와 같이 케라스 전처리 층은 tf.data API와 함께 사용할 수도 있다.\n",
    "map()메서드에 케라스 전처리 층을 적용할 수 있음.\n",
    "'''\n",
    "train_set_normalized = train_set.map(lambda features, label: (norm_layer(features), label)) # 특성만 정규화 층을 통과시킨다.\n",
    "valid_set_normalized = valid_set.map(lambda features, label: (norm_layer(features), label))"
   ],
   "id": "7d8667f737f00190",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "그래서 이 문제를 해결하기 위해서 adapt()메서드를 호출한 Nomalization층과 훈련된 모델을 포함하는 새로운 모델을 만든다.\n",
    "\n",
    "훈련할 때는 Nomalization층을 독립적으로 빼서 정규화를 딱 한 번만 진행하는 식으로 훈련속도를 높이고, 배포할 때 이미 훈련 데이터에 적응시킨 정규화 층을 집어넣어서 배포하는 것이다.\n",
    "\n",
    "이제 훈련 속도도 빠르고 전처리 불일치 문제도 해결했다!!!!!"
   ],
   "id": "402db4d598b73c8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T18:38:27.669415Z",
     "start_time": "2025-02-08T18:38:25.770288Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = tf.keras.Sequential([\n",
    "    norm_layer,\n",
    "    tf.keras.layers.Dense(1, activation=\"relu\")\n",
    "])\n",
    "model.compile(loss=\"mse\", optimizer=\"sgd\")\n",
    "model.fit(train_set, validation_data=valid_set, epochs=1)"
   ],
   "id": "12921c4c3ff2ee77",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    363/Unknown \u001B[1m1s\u001B[0m 3ms/step - loss: 2.4226"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-09 03:38:27.178090: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 8228886785398399008\n",
      "2025-02-09 03:38:27.178104: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 17957140967288023351\n",
      "2025-02-09 03:38:27.178107: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 3076765436023824323\n",
      "2025-02-09 03:38:27.178110: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 15456812406770037240\n",
      "2025-02-09 03:38:27.178112: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 18381312475554204736\n",
      "2025-02-09 03:38:27.178114: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 9752210912658541858\n",
      "2025-02-09 03:38:27.178115: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 206466625880080012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m363/363\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 5ms/step - loss: 2.4198 - val_loss: 0.5100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-09 03:38:27.663741: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 9878487928683836563\n",
      "2025-02-09 03:38:27.663757: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 1632688311992821238\n",
      "2025-02-09 03:38:27.663760: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 8728971235411287552\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x30f7d4cd0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "케라스 전처리 층이 제공하는 것보다 더 많은 기능이 필요하다면 언제든지 자신만의 케라스 층을 만들면 된다!\n",
    "\n",
    "아래는 예시 코드"
   ],
   "id": "82c63c821653b383"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T18:38:27.685853Z",
     "start_time": "2025-02-08T18:38:27.683056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MyNormalization(tf.keras.layers.Layer):\n",
    "    def adapt(self, X):\n",
    "        self.mean_ = np.mean(X, axis=0, keepdims=True)\n",
    "        self.std_ = np.std(X, axis=0, keepdims=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        eps = tf.keras.backend.epsilon()  # 0 나눗셈 방지\n",
    "        return (inputs - self.mean_) / (self.std_ + eps)"
   ],
   "id": "57d11e70f58cd97c",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Discretization, CategoryEncoding, StringLookup, Hashing",
   "id": "9632f14f06ed8814"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "일단 패스하고 임베딩으로 넘어감.",
   "id": "4fdd83abbe7b25c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 임베딩을 사용해서 범주형 특성 인코딩하기",
   "id": "2840b82ce0d66445"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "임베딩은 범주나 어휘 사전의 단어와 같은 고차원 데이터의 밀집 표현이다. 50000개의 범주가 있다면 원-핫 인코딩은 50000차원의 희소벡터를 만든다. 반면 임베딩은 상대적으로 작은 밀집 벡터이다. **벡터의 차원 수는 튜닝해야 할 하이퍼파라미터다.**\n",
    "\n",
    "딥러닝에서 임베딩을 일반적으로 랜덤하게 초기화되고, 다른 모델 파라미터와 함께 경사 하강법으로 훈련된다. (잘 이해가 안될 수 있는데, 얘를 들어서 Embedding 레이어 같은 경우 (단어 수, 차원 수) shape의 임베딩 행렬을 가진다. 그리고 hello라는 단어가 인덱스 1번으로 등록되어 있으면 가중치 행렬의 인덱스 1번 행 가중치를 꺼내주는 식으로 벡터로 변환하는 것이다. 이 가중치들이 처음에는 랜덤으로 초기화되어 있는데, loss가 계산되면서 이 임베딩 행렬이 학습되는 것이다. 혹은 Word2Vec처럼 자기지도학습으로 사전 학습된 임베딩 행렬을 가져와서 사용할 수도 있다.)\n",
    "\n",
    "**임베딩을 훈련할 수 있기 때문에 훈련 도중에 점차 향상**된다. **비슷한 범주들은 경사하강법이 더 가깝게 만든다.**\n",
    "\n",
    "범주가 **유용하게 표현되도록 임베딩이 훈련되는 경향**이 있다. 이를 **표현 학습**이라고 한다."
   ],
   "id": "df200c3f192352f9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "케라스는 임베딩 행렬을 감싼 Embedding 층을 제공한다. 이 행렬은 범주마다 하나의 행을, 임베딩 차원하다 하나의 열을 가진다.\n",
    "\n",
    "기본적으로 랜덤하게 초기화된다. 범주ID를 임베딩으로 변환하기 위해 임베딩 행렬에서 범주에 해당하는 행을 찾아서 반환한다. 이게 전부다."
   ],
   "id": "75be80a5af45eb8f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T18:38:27.742595Z",
     "start_time": "2025-02-08T18:38:27.698696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tf.random.set_seed=42\n",
    "\n",
    "# 이 층은 아직 훈련되지 않았기 때문에 일언 인코딩 값은 그냥 랜덤한 숫자다.\n",
    "embedding_layer = tf.keras.layers.Embedding(input_dim=5, output_dim=2)\n",
    "embedding_layer(np.array([2,4,2]))"
   ],
   "id": "83f774319992edb6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       "array([[-0.03145789, -0.02335962],\n",
       "       [-0.04922844, -0.03540355],\n",
       "       [-0.03145789, -0.02335962]], dtype=float32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "범주형 텍스트 특성을 임베딩하기 위해 `StringLookup`층과 `Embedding`층을 아래 코드와 같이 연결할 수 있다.",
   "id": "ea5de1f0beb61867"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T18:38:27.799420Z",
     "start_time": "2025-02-08T18:38:27.756375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tf.random.set_seed=42\n",
    "\n",
    "ocean_prox = [\"<1H OCEAN\",\"INLAND\", \"NEAR OCEAN\", \"NEAR BAY\", \"ISLAND\"]\n",
    "\n",
    "str_lookup_layer = tf.keras.layers.StringLookup()\n",
    "str_lookup_layer.adapt(ocean_prox) # 어휘사전을 만든다.\n",
    "\n",
    "# 범주형 텍스트 특성을 받아서 벡터로 임베딩하는 모델을 만든다.\n",
    "lookup_and_embed = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(shape=[], dtype=tf.string),\n",
    "    str_lookup_layer,\n",
    "    tf.keras.layers.Embedding(input_dim=str_lookup_layer.vocabulary_size(), output_dim=2)\n",
    "    # 입력 차원은 어휘사전의 크기와 같아야 한다. 단어수+OOV 버킷 수(1개)\n",
    "    # 이 코드에서는 2차원 임베딩을 사용하지만 일반적으로 임베딩은 10~300차원으로 구성된다고 한다. 훈련세트의 크기에 따라 다르므로 하이퍼파라미터로써 튜닝해야 한다.\n",
    "])\n",
    "\n",
    "lookup_and_embed(np.array([\"<1H OCEAN\",\"INLAND\", \"NEAR OCEAN\"]))"
   ],
   "id": "f6dd33aef4b97bea",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       "array([[ 0.03800198,  0.02812399],\n",
       "       [ 0.00636127, -0.02606433],\n",
       "       [-0.02713052, -0.03641682]], dtype=float32)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "이를 모두 연결해서 수치형 특성과 범주형 특성을 입력받아서 처리하는 모델을 만들 수 있다.",
   "id": "e0f7a72152b678a3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T18:38:28.989056Z",
     "start_time": "2025-02-08T18:38:28.965228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_input = tf.keras.layers.Input(shape=[8], name=\"num\")\n",
    "cat_input = tf.keras.layers.Input(shape=[], name=\"cat\", dtype=tf.string)\n",
    "\n",
    "cat_encoded = lookup_and_embed(cat_input)\n",
    "\n",
    "encoded_inputs = tf.keras.layers.concatenate([num_input, cat_encoded])\n",
    "output = tf.keras.layers.Dense(1)(encoded_inputs)\n",
    "\n",
    "model = tf.keras.Model(inputs=[num_input, cat_input], outputs=output)\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"sgd\")"
   ],
   "id": "689a7f42de6660a",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "원-핫 인코딩 다음에 뒤따르는 활성화 함수가 없고, 편향이 없는 Dense층은 Embedding층과 동등한 역할을 한다고 한다.",
   "id": "41c5e911a5734485"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 텍스트 전처리",
   "id": "7506a7677c6eae10"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "케라스는 기본적인 텍스트 전처리를 위한 TextVectorization 층을 제공한다. StringLookup 층과 매우 비슷하게 층을 만들 때 vocabulary 매개변수로 어휘 사전을 전달하거나, adapt() 메서드를 사용해서 훈련데이터로부터 어휘 사전을 학습할 수 있다.",
   "id": "5a5e87fb028cd032"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T18:38:29.139603Z",
     "start_time": "2025-02-08T18:38:29.095152Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_data = [\"To be\", \"!(to be)\", \"That's the question\", \"Be, be, be.\"]\n",
    "\n",
    "text_vec_layer = tf.keras.layers.TextVectorization()\n",
    "text_vec_layer.adapt(train_data)\n",
    "print(text_vec_layer.get_vocabulary())\n",
    "text_vec_layer([\"Be good!\", \"Question: be or be?\"])"
   ],
   "id": "4b4b8780d9e46f15",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'be', 'to', 'the', 'thats', 'question']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 4), dtype=int64, numpy=\n",
       "array([[2, 1, 0, 0],\n",
       "       [6, 2, 1, 2]])>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "매개변수 ouput_mode=\"tf_idf\"로 설정하면 훈련 데이터에 자주 등장하는 단어의 가중치는 높이고, 드물게 등장하는 단어의 가중치를 높인다.\n",
    "\n",
    "해보려고 했는데, 단어의 수가 너무 적다고 안됨..."
   ],
   "id": "b6a9c597afbf5e4c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T18:38:29.612170Z",
     "start_time": "2025-02-08T18:38:29.211416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text_vec_layer = tf.keras.layers.TextVectorization(output_mode=\"tf_idf\")\n",
    "text_vec_layer.adapt(train_data)\n",
    "text_vec_layer([\"Be good!\", \"Question: be or be?\"])"
   ],
   "id": "3b8c12ba33893277",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 6), dtype=float32, numpy=\n",
       "array([[0.96725637, 0.6931472 , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.96725637, 1.3862944 , 0.        , 0.        , 0.        ,\n",
       "        1.0986123 ]], dtype=float32)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "이런 텍스트 인코딩 방법은 몇가지 중요한 제약사항이 있다.\n",
    "\n",
    "공백으로 단어가 구분되는 언어에만 사용 가능. 동음이의어를 구별하지 못함. 단어의 관계에 대한 힌트를 모델에게 주지 못함. 단어의 순서가 사라진다.\n",
    "\n",
    "다른 좋은 방법은 훨씬 고급의 텍스트 전처리 기능을 제공하는 텐서플로 텍스트 라이브러리를 사용하는 것이다. 예를 들어서 텍스트를 단어보다 작은 토큰으로 분할할 수 있는 부분 단어 토크나이저를 제공한다."
   ],
   "id": "b523c06ccfc2741c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 사전 훈련된 언어 모델 구성 요소 사용하기",
   "id": "406f72e5c7615d3e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "이건 맛보기만 하자. 원시 텍스트를 입력 받아 50차원 문장 임베딩을 출력하는 모듈이다. 문자열을 입력 받아서 하나의 임베딩을 출력한다.\n",
    "\n",
    "내부적으로 문자열을 파싱하고, 대규모 말뭉치에서 사전 훈련된 임베딩 행렬을 사용해 각 단어를 임베딩한다.\n",
    "그리고 모든 단어 임베딩의 평균을 계산하면 그 결과가 문장 임베딩이다.\n",
    "\n",
    "처음에 이상한 오류가 나면서 안됐었는데 껐다가 켜니까 되네ㅋㅋ"
   ],
   "id": "b547d22749816f40"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T18:39:13.965623Z",
     "start_time": "2025-02-08T18:39:13.689832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "embed = hub.load(\"https://www.kaggle.com/models/google/nnlm/TensorFlow2/en-dim50/1\")\n",
    "embeddings = embed([\"cat is on the mat\", \"dog is in the fog\"])\n",
    "print(embeddings)"
   ],
   "id": "1564edec9e658720",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.16589954  0.0254965   0.1574857   0.17688066  0.02911299 -0.03092718\n",
      "   0.19445257 -0.05709129 -0.08631689 -0.04391516  0.13032274  0.10905275\n",
      "  -0.08515751  0.01056632 -0.17220995 -0.17925954  0.19556305  0.0802278\n",
      "  -0.03247919 -0.49176937 -0.07767699 -0.03160921 -0.13952136  0.05959712\n",
      "   0.06858718  0.22386682 -0.16653948  0.19412343 -0.05491862  0.10997339\n",
      "  -0.15811177 -0.02576607 -0.07910853 -0.258499   -0.04206644 -0.20052543\n",
      "   0.1705603  -0.15314153  0.0039225  -0.28694248  0.02468278  0.11069503\n",
      "   0.03733957  0.01433943 -0.11048374  0.11931834 -0.11552787 -0.11110869\n",
      "   0.02384969 -0.07074881]\n",
      " [ 0.1437864   0.08291595  0.10897306  0.04464385 -0.03630389 -0.12605834\n",
      "   0.20263346  0.12862863 -0.07873426 -0.01195358  0.0020956  -0.03080653\n",
      "  -0.08019945 -0.18797135 -0.11973457 -0.26926652  0.05157408 -0.15541205\n",
      "  -0.12221853 -0.27182642  0.08750801 -0.05013347  0.03012378  0.2053423\n",
      "   0.10000334  0.18292566 -0.18280756  0.0780353   0.10936535 -0.10147726\n",
      "  -0.19995196  0.0398768  -0.15377024 -0.1095404  -0.18498933 -0.15899731\n",
      "   0.0558111   0.15251887  0.02046264 -0.25878936 -0.13057052  0.0782799\n",
      "   0.04044291  0.14456013  0.00264394  0.1399635  -0.04803645 -0.17253871\n",
      "  -0.03153546  0.09077   ]], shape=(2, 50), dtype=float32)\n"
     ]
    }
   ],
   "execution_count": 36
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
